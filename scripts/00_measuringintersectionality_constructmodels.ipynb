{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec on DocuSouth Corpus\n",
    "\n",
    "This script create the word embedding model on the full DocSouth corpus, as well as the 40 randomly created models, for use in constructing the confidence intervals, used in the paper \"Leveraging the Alignment between Machine Learning and Intersectionality: Using Word Embeddings to Measure Intersectional Experiences of the Nineteenth Century U.S. South.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "\n",
    "#Data Wrangling\n",
    "import pandas\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from random import choices\n",
    "\n",
    "import gensim #library needed for word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(pathname):\n",
    "    allFiles = os.listdir(pathname)\n",
    "    allFiles = [pathname+file for file in allFiles]\n",
    "    return(allFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_tokenize(text):\n",
    "    \n",
    "    # Get a list of punctuation marks\n",
    "    punct = string.punctuation + '“' + '”' + '‘' + \"’\"\n",
    "    \n",
    "    lower_case = text.lower()\n",
    "    lower_case = lower_case.replace('—', ' ').replace('\\n', ' ')\n",
    "    \n",
    "    # Iterate through text removing punctuation characters\n",
    "    no_punct = \"\".join([char for char in lower_case if char not in punct])\n",
    "    \n",
    "    # Split text over whitespace into list of words\n",
    "    tokens = no_punct.split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import and Pre-Processing\n",
    "\n",
    "### Corpus Description\n",
    "\n",
    "The corpus description can be found [here](https://docsouth.unc.edu/docsouthdata/).\n",
    "\n",
    "### Import Data\n",
    "\n",
    "Read in all of the .txt files in two folders, do some pre-processing on it, and concat them all into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_fpn = pandas.read_csv('../data/first-person-narratives-american-south/data/toc.csv', encoding = 'utf-8')\n",
    "meta_neh = pandas.read_csv('../data/na-slave-narratives/data/toc.csv', encoding = 'utf-8')\n",
    "meta = pandas.concat([meta_fpn, meta_neh]).reset_index()\n",
    "\n",
    "#Dropping multiple editions from the same autobiography\n",
    "#Keeping the autobiography with the latest date\n",
    "\n",
    "#349 and 270 = Frederick Douglass\n",
    "#363 = William Wells Brown\n",
    "meta.drop([349, 270, 363], inplace=True)\n",
    "\n",
    "meta.drop_duplicates(subset = 'Filename', inplace=True)\n",
    "\n",
    "#read in all the data, with some cleaning\n",
    "path_fpn = get_path('../data/first-person-narratives-american-south/data/texts/') # indicate the local path where files are stored\n",
    "path_neh = get_path('../data/na-slave-narratives/data/texts/')\n",
    "path_all = path_fpn + path_neh\n",
    "\n",
    "#remove duplicate files and multiple editions of same narrative\n",
    "keep = meta['Filename'].tolist()\n",
    "keep = [name.replace('.xml', '.txt') for name in keep]\n",
    "filenames = []\n",
    "path = []\n",
    "\n",
    "for p in path_all:\n",
    "    if (p.split('/')[-1] not in filenames) and (p.split('/')[-1] in keep):\n",
    "        filenames.append(p.split('/')[-1])\n",
    "        path.append(p)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in path:\n",
    "    with open(file, encoding='utf-8') as myfile:\n",
    "        data.append(myfile.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "\n",
    "Word2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. In this text there is no punctuation, and thus nothing resembling a sentence. In other text we  want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\n",
    "\n",
    "You can split your text in sentences using ` nltk.tokenize.sent_tokenize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title', 'page', 'image', 'introduction']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [sentence for text in data for sentence in sent_tokenize(text)]\n",
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]\n",
    "words_by_sentence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word2Vec\n",
    "\n",
    "### Word Embedding\n",
    "Word2Vec is the most prominent word embedding algorithm. Word embedding generally attempts to identify semantic relationships between words by observing them in context.\n",
    "\n",
    "Imagine that each word in a novel has its meaning determined by the ones that surround it in a limited window. For example, in Moby Dick's first sentence, “me” is paired on either side by “Call” and “Ishmael.” After observing the windows around every word in the novel (or many novels), the computer will notice a pattern in which “me” falls between similar pairs of words to “her,” “him,” or “them.” Of course, the computer had gone through a similar process over the words “Call” and “Ishmael,” for which “me” is reciprocally part of their contexts.  This chaining of signifiers to one another mirrors some of humanists' most sophisticated interpretative frameworks of language.\n",
    "\n",
    "The two main flavors of Word2Vec are CBOW (Continuous Bag of Words) and Skip-Gram, which can be distinguished partly by their input and output during training. Skip-Gram takes a word of interest as its input (e.g. \"me\") and tries to learn how to predict its context words (\"Call\",\"Ishmael\"). CBOW does the opposite, taking the context words (\"Call\",\"Ishmael\") as a single input and tries to predict the word of interest (\"me\").\n",
    "\n",
    "In general, CBOW is is faster and does well with frequent words, while Skip-Gram potentially represents rare words better.\n",
    "\n",
    "### Word2Vec Features\n",
    "<ul>\n",
    "<li>Size: Number of dimensions for word embedding model</li>\n",
    "<li>Window: Number of context words to observe in each direction</li>\n",
    "<li>min_count: Minimum frequency for words included in model</li>\n",
    "<li>sg (Skip-Gram): '0' indicates CBOW model; '1' indicates Skip-Gram</li>\n",
    "<li>Alpha: Learning rate (initial); prevents model from over-correcting, enables finer tuning</li>\n",
    "<li>Iterations: Number of passes through dataset</li>\n",
    "<li>Batch Size: Number of words to sample from data during each pass</li>\n",
    "<li>Worker: Set the 'worker' option to ensure reproducibility</li>\n",
    "</ul>\n",
    "\n",
    "Note: Script uses default value for each argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, or fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(words_by_sentence, size=100, window=5,\n",
    "                               min_count=10, sg=1, alpha=0.025, iter=5, batch_words=10000, workers=1)\n",
    "\n",
    "# Save model for later use\n",
    "model.wv.save_word2vec_format('../data/word2vec_all_clean.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 40 random models for constructing confidence intervals\n",
    "\n",
    "def gen_model(words_by_sentence, num):\n",
    "    \"\"\"\n",
    "    Takes a list of words by senence as input and a number (for naming the file)\n",
    "    Saves a word2vec model in the word2vec_robust folder\n",
    "    \"\"\"\n",
    "\n",
    "    model = gensim.models.Word2Vec(words_by_sentence, size=100, window=5,\n",
    "                                   min_count=10, sg=1, alpha=0.025, iter=5, batch_words=10000, workers=1)\n",
    "    \n",
    "    \n",
    "    model.wv.save_word2vec_format('../data/word2vec_robust/model%d.txt' % num)\n",
    "    \n",
    "#Number of sentences, for use in creating random sentences\n",
    "num_sent = len(words_by_sentence)\n",
    "\n",
    "for num in range(0,40):\n",
    "    print(num)\n",
    "    \n",
    "    #extract random sample of sentences with replacement, \n",
    "    #equal to total number of sentences in the full corpus\n",
    "    gen_model(choices(words_by_sentence, k = num_sent), num)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
